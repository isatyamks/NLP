{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0edc664d-3409-407a-94e0-81dd236d812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9a75bb-fd25-4f03-964e-64ea92d4f528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\isatyamks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\isatyamks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\isatyamks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3420b6eb-4991-4d66-afcd-2ebd67de4b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Tokenizes the text into sentences and cleans each sentence.\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    clean_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words and word not in string.punctuation]\n",
    "        clean_sentences.append(' '.join(filtered_words))\n",
    "\n",
    "    return sentences, clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30d5e1d8-ac05-4c03-83d2-009ef5370e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(sentences):\n",
    "    \"\"\"Extracts keywords from sentences using TF-IDF.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=5)\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    keywords = vectorizer.get_feature_names_out()\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5505fd09-ef75-4f58-acbe-6634e228e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(sentence):\n",
    "    \"\"\"Generates questions based on simple syntactic rules.\"\"\"\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    questions = []\n",
    "        # Rule 1: If sentence has a proper noun (NNP), form a 'What' question\n",
    "    for i, (word, tag) in enumerate(pos_tags):\n",
    "        if tag == 'NNP':\n",
    "            question = sentence.replace(word, f\"What is {word}?\")\n",
    "            questions.append(question)\n",
    "            break\n",
    "\n",
    "    # Rule 2: If sentence starts with a verb, form a 'How' question\n",
    "    if pos_tags[0][1].startswith('VB'):\n",
    "        question = f\"How does {sentence}?\"\n",
    "        questions.append(question)\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcb76cf0-e4e9-49ea-9756-1777966f363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_questions(questions, keywords):\n",
    "    \"\"\"Ranks questions based on relevance to extracted keywords.\"\"\"\n",
    "    ranked = []\n",
    "    for question in questions:\n",
    "        score = sum(1 for word in word_tokenize(question) if word.lower() in keywords)\n",
    "        ranked.append((question, score))\n",
    "\n",
    "    # Sort by score\n",
    "    ranked = sorted(ranked, key=lambda x: x[1], reverse=True)\n",
    "    return [q for q, _ in ranked[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1771b834-7a47-47b3-b16a-55429377f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(text):\n",
    "    \"\"\"Processes the input text and returns the top 5 questions.\"\"\"\n",
    "    sentences, clean_sentences = preprocess_text(text)\n",
    "    keywords = extract_keywords(clean_sentences)\n",
    "    all_questions = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        questions = generate_questions(sentence)\n",
    "        all_questions.extend(questions)\n",
    "\n",
    "    top_questions = rank_questions(all_questions, keywords)\n",
    "    return top_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1799f5f6-0e83-4eef-af29-d586340bd808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Questions:\n",
      "1. My name is What is Satyam? and i am a computer science studenti live in bihar which is a state of IndiaI love machine learning and artificial intelligence so much\n"
     ]
    }
   ],
   "source": [
    "paragraph = (\n",
    "        \"My name is Satyam and i am a computer science student\"\n",
    "        \"i live in bihar which is a state of India\"\n",
    "        \"I love machine learning and artificial intelligence so much\"\n",
    "    )\n",
    "\n",
    "questions = main(paragraph)\n",
    "print(\"Top 5 Questions:\")\n",
    "for i, q in enumerate(questions, 1):\n",
    "    print(f\"{i}. {q}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d37abea9-7bba-4739-8305-3754742e8789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\isatyamks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "085c56aa-0d9c-425f-bb85-01bae28b96d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\isatyamks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abf00503-74a0-45ff-9a8e-11f167effbe5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5ForConditionalGeneration, T5Tokenizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4c42bbd-faa3-4472-9cf9-49603bed3bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.5.1+cpu)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (70.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\isatyamks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.3/10.1 MB 5.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.6/10.1 MB 5.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.9/10.1 MB 4.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.7/10.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.5/10.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.3/10.1 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.6/10.1 MB 4.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.4/10.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.9/10.1 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.1 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 4.2 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.8/2.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 4.7 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.23.4\n",
      "    Uninstalling huggingface-hub-0.23.4:\n",
      "      Successfully uninstalled huggingface-hub-0.23.4\n",
      "Successfully installed huggingface-hub-0.26.5 safetensors-0.4.5 tokenizers-0.21.0 transformers-4.47.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tansformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d888c976-2dff-40aa-bb6f-38e8f214e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42f6a6b0-ad1a-4f0e-a0af-c722d378416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\" \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ba587b5-ad59-4aa0-8f62-a0c0273b5691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_t5(paragraph, max_questions=5):\n",
    "    \"\"\"Generates questions using the T5 model.\"\"\"\n",
    "    # Prepare the input for the model\n",
    "    input_text = f\"generate questions: {paragraph}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate output using the model\n",
    "    outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=128,\n",
    "    num_beams=10,  # Increase beam search width\n",
    "    temperature=0.7,  # Encourage more creative outputs\n",
    "    top_p=0.9,  # Nucleus sampling for diverse outputs\n",
    "    num_return_sequences=max_questions\n",
    "    )\n",
    "\n",
    "\n",
    "    # Decode and format the output\n",
    "    questions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8edc146-4304-4e06-b4c3-34a8cb076dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Questions:\n",
      "1. machine learning: Machine learning is a subset of artificial intelligence that enables systems to learn patterns from data and make predictions. It is widely used in various domains like healthcare, finance, and technology. Supervised, unsupervised, and reinforcement learning are the primary categories of machine learning.\n",
      "2. questions: Machine learning is a subset of artificial intelligence that enables systems to learn patterns from data and make predictions. It is widely used in various domains like healthcare, finance, and technology. Supervised, unsupervised, and reinforcement learning are the primary categories of machine learning.\n",
      "3. : Machine learning is a subset of artificial intelligence that enables systems to learn patterns from data and make predictions. It is widely used in various domains like healthcare, finance, and technology. Supervised, unsupervised, and reinforcement learning are the primary categories of machine learning.\n",
      "4. questions: Machine learning is a subset of artificial intelligence that enables systems to learn patterns from data and make predictions. Machine learning is widely used in various domains like healthcare, finance, and technology. Supervised, unsupervised, and reinforcement learning are the primary categories of machine learning.\n",
      "5. : Machine learning is a subset of artificial intelligence that enables systems to learn patterns from data and make predictions. Machine learning is widely used in various domains like healthcare, finance, and technology. Supervised, unsupervised, and reinforcement learning are the primary categories of machine learning.\n"
     ]
    }
   ],
   "source": [
    "pqaragraph = (\n",
    "        \"Machine learning is a subset of artificial intelligence that enables systems to learn patterns from data and make predictions. \"\n",
    "        \"It is widely used in various domains like healthcare, finance, and technology. \"\n",
    "        \"Supervised, unsupervised, and reinforcement learning are the primary categories of machine learning.\")\n",
    "questions = generate_questions_t5(pqaragraph)\n",
    "print(\"Top Questions:\")\n",
    "for i, q in enumerate(questions, 1):\n",
    "    print(f\"{i}. {q}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad489a8-71c6-4362-b9a9-a376b6e99127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a02a831-61f5-4246-b7a7-a1fb66340f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
